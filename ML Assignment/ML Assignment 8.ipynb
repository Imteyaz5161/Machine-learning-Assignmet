{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8a2d06",
   "metadata": {},
   "source": [
    "# ML Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c4342",
   "metadata": {},
   "source": [
    "## 1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270858ab",
   "metadata": {},
   "source": [
    "__Ans:__ Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a521c",
   "metadata": {},
   "source": [
    "## 2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d9027",
   "metadata": {},
   "source": [
    "__Ans:__ The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c64bc",
   "metadata": {},
   "source": [
    "## 3. Describe how nominal variables are encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8826da",
   "metadata": {},
   "source": [
    "__Ans:__ Nominal data is made of discrete values with no numerical relationship between the different categories — mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish. Ordinal or Label Encoding can be used to transform non-numerical labels into numerical labels (or nominal categorical variables). Numerical labels are always between 1 and the number of classes. The labels chosen for the categories have no relationship. So categories that have some ties or are close to each other lose such information after encoding. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1445b05",
   "metadata": {},
   "source": [
    "## 4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c50bb4",
   "metadata": {},
   "source": [
    "__Ans:__ Numeric Features can be converted to Categorical Features using Binning. Discretization: It is the process of transforming continuous variables into categorical variables by creating a set of intervals, which are contiguous, that span over the range of the variable’s values. It is also known as “Binning”, where the bin is an analogous name for an interval.\n",
    "\n",
    "Benefits of this method are:\n",
    "\n",
    "1. Handles the Outliers in a better way.\n",
    "2. Improves the value spread.\n",
    "3. Minimize the effects of small observation errors.\n",
    "\n",
    "ables.\n",
    "\n",
    "**Techniques to Encode Numerical Columns:**\n",
    "\n",
    "(a) `Equal width binning:` It is also known as “Uniform Binning” since the width of all the intervals is the same. The algorithm divides the data into N intervals of equal size. The width of intervals is:\n",
    "\n",
    "w=(max-min)/N\n",
    "\n",
    "Therefore, the interval boundaries are:[min+w], [min+2w], [min+3w],..., [min+(N-1)w] where, min and max are the minimum and maximum value from the data respectively. This technique does not changes the spread of the data but does handle the outliers.\n",
    "\n",
    "(b) `Equal frequency binning:` It is also known as “Quantile Binning”. The algorithm divides the data into N groups where each group contains approximately the same number of values.\n",
    "\n",
    "Consider, we want 10 bins, that is each interval contains 10% of the total observations. Here the width of the interval need not necessarily be equal. Handles outliers better than the previous method and makes the value spread approximately uniform(each interval contains almost the same number of values).\n",
    "\n",
    "(c) `K-means binning:` This technique uses the clustering algorithm namely ” K-Means Algorithm”. This technique is mostly used when our data is in the form of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca3d42",
   "metadata": {},
   "source": [
    "## 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792297e6",
   "metadata": {},
   "source": [
    "__Ans:__ Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694415ec",
   "metadata": {},
   "source": [
    "## 6. When is a feature considered irrelevant? What can be said to quantify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cd9c0",
   "metadata": {},
   "source": [
    "__Ans:__ Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree).\n",
    "\n",
    "p-value greater than 0.05 means that the feature is insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ab9ea",
   "metadata": {},
   "source": [
    "## 7. When is a function considered redundant? What criteria are used to identify features that could be redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8120c",
   "metadata": {},
   "source": [
    "__Ans:__ If two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e8167",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/e4b387ddf91fb6ca78b59a8bc9d8fa33d08c5f4060ab3c1dddd654f40aeb4ae7/68747470733a2f2f736c696465706c617965722e636f6d2f736c6964652f343339343634342f31342f696d616765732f332f4261636b67726f756e642b52656c6576616e63652b6265747765656e2b66656174757265732b436f7272656c6174696f6e2b462d7374617469737469632e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770641b",
   "metadata": {},
   "source": [
    "## 8. What are the various distance measurements used to determine feature similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65bd59",
   "metadata": {},
   "source": [
    "__Ans:__ Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "\n",
    "1. __Hamming Distance:__ Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n",
    "\n",
    "2. __Euclidean Distance:__ Calculates the distance between two real-valued vectors.\n",
    "\n",
    "3. __Manhattan Distance:__ Also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n",
    "\n",
    "4. __Minkowski Distance:__ Minkowski distance calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217b651",
   "metadata": {},
   "source": [
    "## 9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58289609",
   "metadata": {},
   "source": [
    "__Ans:__ Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411da14",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/c7f6b7b3d4afe2d21354649c6ee195353774af7e93e9429ba9ef81228295f09a/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e643947635166486a3055694646767534697a366c34736a696e4b773131427442676a70416a69547726757371703d434155)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2e447",
   "metadata": {},
   "source": [
    "## 10. Distinguish between feature transformation and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab120fe9",
   "metadata": {},
   "source": [
    "**Ans** Distinction between Feature transformation and feature selection:\n",
    "\n",
    "**Feature Transformation:**\n",
    "- Feature transformation involves creating new features by applying mathematical or statistical transformations to the existing features.\n",
    "\n",
    "- It aims to convert the original features into a new representation that captures the underlying patterns or relationships in the data.\n",
    "\n",
    "- Examples of feature transformation techniques include scaling, normalization, log transformations, polynomial transformations, and dimensionality reduction methods like Principal Component Analysis (PCA) or t-SNE.\n",
    "\n",
    "- Feature transformation can help improve the performance of machine learning models by making the data more suitable for modeling, enhancing interpretability, reducing noise, or addressing issues like multicollinearity.\n",
    "\n",
    "**Feature Selection:**\n",
    "- Feature selection involves selecting a subset of the original features from the dataset to build a model.\n",
    "\n",
    "- It aims to identify the most relevant features that have the most predictive power for the target variable.\n",
    "\n",
    "- Feature selection methods assess the importance or usefulness of each feature and eliminate irrelevant or redundant features.\n",
    "\n",
    "- Examples of feature selection techniques include filter methods (e.g., correlation-based feature selection), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., Lasso regularization).\n",
    "\n",
    "- Feature selection can help reduce overfitting, improve model performance, simplify models, reduce computational complexity, and enhance interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4643c30",
   "metadata": {},
   "source": [
    "## 11. Make brief notes on any two of the following:\n",
    "\n",
    "- 1. SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "- 2. Collection of features using a hybrid approach\n",
    "\n",
    "- 3. The width of the silhouette\n",
    "\n",
    "- 4. Receiver operating characteristic curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663b1ac",
   "metadata": {},
   "source": [
    "__Ans__\n",
    "\n",
    "**1. SVD (Singular Value Decomposition):**\n",
    "- SVD is a matrix factorization technique that decomposes a matrix into three matrices: U, Σ, and V^T.\n",
    "\n",
    "- It is widely used in various applications, including dimensionality reduction, data compression, image processing, and collaborative filtering.\n",
    "\n",
    "- SVD allows capturing the most important patterns and relationships in the data by identifying the singular values and corresponding singular vectors.\n",
    "\n",
    "- It can be used for low-rank approximation, where the matrix is approximated using only the most significant singular values and vectors.\n",
    "\n",
    "- SVD is particularly useful in recommendation systems, where it helps in identifying latent factors and making personalized recommendations based on user-item interactions.\n",
    "\n",
    "**4. Receiver Operating Characteristic (ROC) Curve:**\n",
    "- ROC curve is a graphical representation of the performance of a binary classification model.\n",
    "\n",
    "- It plots the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds.\n",
    "\n",
    "- The ROC curve provides a trade-off analysis between the model's sensitivity (recall) and specificity (1 - FPR).\n",
    "\n",
    "- It allows evaluating the performance of a classifier across different threshold settings and helps in selecting an appropriate threshold based on the desired balance between TPR and FPR.\n",
    "\n",
    "- The area under the ROC curve (AUC-ROC) is a commonly used metric to quantify the overall performance of the classifier.\n",
    "\n",
    "- AUC-ROC ranges between 0 and 1, where a value closer to 1 indicates a better-performing classifier with higher discrimination capability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493f0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "306px",
    "width": "751px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
